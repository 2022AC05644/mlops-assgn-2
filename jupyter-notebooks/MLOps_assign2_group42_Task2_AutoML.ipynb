{"cells":[{"cell_type":"markdown","id":"f4af85aa-1af9-4c23-bd83-50882252dde5","metadata":{"id":"f4af85aa-1af9-4c23-bd83-50882252dde5"},"source":["## **MLOPs Assignment 2 - Group 42**\n","\n","<b>Group members</b>\n","<ol>\n","    <li>ARUN KUMAR MOHANDAS – 2022AC05190</li>\n","    <li>DEBAYAN MITRA– 2022AC05222</li>\n","    <li>ILYAS MOHD– 2022AC05644</li>\n","    <li>PRITAM MONDAL– 2022AC05090</li>\n","    <li>RONIT MONDAL– 2022AA05142</li>\n"," </ol>"]},{"cell_type":"markdown","id":"47e59669-bb1a-4a43-806b-2d4c6ca53fe8","metadata":{"id":"47e59669-bb1a-4a43-806b-2d4c6ca53fe8"},"source":["# **PROBLEM STATEMENT**\n","\n","**Task 2**:\n","\n","**Model Selection, Training, and Hyperparameter Tuning (6 Marks):**\n","\n","**•Task: Train multiple models, tune hyperparameters, and select the best-performing model.**\n","\n","**•Details: Utilize tools like AutoML, KizenML, or others for model selection and hyperparameter tuning. Document the experimentation process and justify your model choice.**"]},{"cell_type":"markdown","id":"c4b8ca34","metadata":{"id":"c4b8ca34"},"source":["### Opting to use **PyCaret** library:\n","\n","In this notebook, we will utilize **PyCaret** to perform **Automated Machine Learning (AutoML)** for **model selection** and **hyperparameter tuning**. Our goal is to train multiple models and tune their hyperparameters to **select the best-performing model** for the **Pima Indians Diabetes** dataset.\n"]},{"cell_type":"markdown","id":"f9aa1c32-685c-4639-962c-08133746a4fa","metadata":{"id":"f9aa1c32-685c-4639-962c-08133746a4fa"},"source":["### Justification for Using PyCaret for Model Selection and Hyperparameter Tuning\n","\n","**Problem Statement**:  \n","The task requires training multiple models, tuning hyperparameters, and selecting the best-performing model based on the results. Given the need for efficient experimentation and automation in this process, **PyCaret** has been chosen as the tool for model selection and hyperparameter tuning.\n","\n","**Why PyCaret?**\n","\n","1. **All-in-One Machine Learning Library**:  \n","   PyCaret simplifies the entire machine learning workflow by providing a unified and intuitive interface for tasks such as data preprocessing, model training, evaluation, and hyperparameter tuning. This makes it an ideal tool for the quick iteration and experimentation required in model selection and tuning.\n","\n","2. **Automated Machine Learning (AutoML) Capability**:  \n","   PyCaret is designed with AutoML features that automatically train multiple machine learning models and tune their hyperparameters in just a few lines of code. This accelerates the process of finding the best-performing model without manually setting up individual models or tuning procedures.\n","\n","3. **Extensive Model Selection**:  \n","   PyCaret supports over 25 machine learning algorithms across various tasks such as classification, regression, clustering, and more. For this task, it allows us to easily experiment with a variety of models (e.g., decision trees, random forests, gradient boosting, etc.) and select the one with the best performance on the diabetes dataset.\n","\n","4. **Built-in Hyperparameter Tuning**:  \n","   PyCaret includes built-in methods for automatic hyperparameter tuning using grid search and random search. This removes the complexity of manually configuring and experimenting with hyperparameters, saving time and effort during model optimization.\n","\n","5. **User-Friendly and Efficient**:  \n","   PyCaret is designed to be accessible even for users with limited experience in machine learning, as it abstracts away the complexity of writing long code snippets. Additionally, it is highly efficient, allowing for fast experimentation and rapid prototyping, which aligns with the goals of this task.\n","\n","6. **Performance Comparison and Model Interpretation**:  \n","   PyCaret provides comprehensive comparison metrics and visualizations to evaluate the performance of different models side by side. It also offers model interpretation tools like feature importance and SHAP values, helping in the justification of the final model choice.\n","\n","7. **Seamless Integration**:  \n","   PyCaret integrates easily with common data science tools such as pandas, scikit-learn, and Jupyter notebooks. This ensures a smooth workflow from data preprocessing to model deployment.\n","\n","**Conclusion**:  \n","Given its comprehensive set of features, ease of use, and AutoML capabilities, PyCaret is an excellent choice for this task. It allows for efficient model training, hyperparameter tuning, and selection of the best-performing model, which is essential for achieving optimal performance on the diabetes dataset.\n"]},{"cell_type":"code","execution_count":1,"id":"eiRkxQbVHC-U","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eiRkxQbVHC-U","outputId":"28f468a8-0ae1-491d-8644-627bbbe9e2b8","executionInfo":{"status":"ok","timestamp":1727113988033,"user_tz":-180,"elapsed":48646,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pycaret\n","  Downloading pycaret-3.3.2-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: ipython>=5.5.0 in /usr/local/lib/python3.10/dist-packages (from pycaret) (7.34.0)\n","Requirement already satisfied: ipywidgets>=7.6.5 in /usr/local/lib/python3.10/dist-packages (from pycaret) (7.7.1)\n","Requirement already satisfied: tqdm>=4.62.0 in /usr/local/lib/python3.10/dist-packages (from pycaret) (4.66.5)\n","Requirement already satisfied: numpy<1.27,>=1.21 in /usr/local/lib/python3.10/dist-packages (from pycaret) (1.26.4)\n","Requirement already satisfied: pandas<2.2.0 in /usr/local/lib/python3.10/dist-packages (from pycaret) (2.1.4)\n","Requirement already satisfied: jinja2>=3 in /usr/local/lib/python3.10/dist-packages (from pycaret) (3.1.4)\n","Collecting scipy<=1.11.4,>=1.6.1 (from pycaret)\n","  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m664.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting joblib<1.4,>=1.2.0 (from pycaret)\n","  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n","Requirement already satisfied: scikit-learn>1.4.0 in /usr/local/lib/python3.10/dist-packages (from pycaret) (1.5.2)\n","Collecting pyod>=1.1.3 (from pycaret)\n","  Downloading pyod-2.0.2.tar.gz (165 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.8/165.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: imbalanced-learn>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from pycaret) (0.12.3)\n","Collecting category-encoders>=2.4.0 (from pycaret)\n","  Downloading category_encoders-2.6.3-py2.py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: lightgbm>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pycaret) (4.5.0)\n","Requirement already satisfied: numba>=0.55.0 in /usr/local/lib/python3.10/dist-packages (from pycaret) (0.60.0)\n","Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from pycaret) (2.32.3)\n","Requirement already satisfied: psutil>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from pycaret) (5.9.5)\n","Requirement already satisfied: markupsafe>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from pycaret) (2.1.5)\n","Requirement already satisfied: importlib-metadata>=4.12.0 in /usr/local/lib/python3.10/dist-packages (from pycaret) (8.5.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pycaret) (5.10.4)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from pycaret) (2.2.1)\n","Collecting deprecation>=2.1.0 (from pycaret)\n","  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n","Collecting xxhash (from pycaret)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Requirement already satisfied: matplotlib<3.8.0 in /usr/local/lib/python3.10/dist-packages (from pycaret) (3.7.1)\n","Collecting scikit-plot>=0.3.7 (from pycaret)\n","  Downloading scikit_plot-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: yellowbrick>=1.4 in /usr/local/lib/python3.10/dist-packages (from pycaret) (1.5)\n","Requirement already satisfied: plotly>=5.14.0 in /usr/local/lib/python3.10/dist-packages (from pycaret) (5.24.1)\n","Collecting kaleido>=0.2.1 (from pycaret)\n","  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n","Collecting schemdraw==0.15 (from pycaret)\n","  Downloading schemdraw-0.15-py3-none-any.whl.metadata (2.2 kB)\n","Collecting plotly-resampler>=0.8.3.1 (from pycaret)\n","  Downloading plotly_resampler-0.10.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: statsmodels>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pycaret) (0.14.3)\n","Collecting sktime==0.26.0 (from pycaret)\n","  Downloading sktime-0.26.0-py3-none-any.whl.metadata (29 kB)\n","Collecting tbats>=1.1.3 (from pycaret)\n","  Downloading tbats-1.1.3-py3-none-any.whl.metadata (3.8 kB)\n","Collecting pmdarima>=2.0.4 (from pycaret)\n","  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n","Collecting wurlitzer (from pycaret)\n","  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sktime==0.26.0->pycaret) (24.1)\n","Collecting scikit-base<0.8.0 (from sktime==0.26.0->pycaret)\n","  Downloading scikit_base-0.7.8-py3-none-any.whl.metadata (8.8 kB)\n","Collecting scikit-learn>1.4.0 (from pycaret)\n","  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category-encoders>=2.4.0->pycaret) (0.5.6)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn>=0.12.0->pycaret) (3.5.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.12.0->pycaret) (3.20.2)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->pycaret) (71.0.4)\n","Collecting jedi>=0.16 (from ipython>=5.5.0->pycaret)\n","  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->pycaret) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->pycaret) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->pycaret) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->pycaret) (3.0.47)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->pycaret) (2.18.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->pycaret) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->pycaret) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->pycaret) (4.9.0)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.6.5->pycaret) (5.5.6)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.6.5->pycaret) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.6.5->pycaret) (3.6.9)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.6.5->pycaret) (3.0.13)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.8.0->pycaret) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.8.0->pycaret) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.8.0->pycaret) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.8.0->pycaret) (1.4.7)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.8.0->pycaret) (10.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.8.0->pycaret) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.8.0->pycaret) (2.8.2)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=4.2.0->pycaret) (2.20.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=4.2.0->pycaret) (4.23.0)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat>=4.2.0->pycaret) (5.7.2)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55.0->pycaret) (0.43.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.0->pycaret) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.0->pycaret) (2024.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.14.0->pycaret) (9.0.0)\n","Collecting dash>=2.9.0 (from plotly-resampler>=0.8.3.1->pycaret)\n","  Downloading dash-2.18.1-py3-none-any.whl.metadata (10 kB)\n","Collecting orjson<4.0.0,>=3.8.0 (from plotly-resampler>=0.8.3.1->pycaret)\n","  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m916.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tsdownsample>=0.1.3 (from plotly-resampler>=0.8.3.1->pycaret)\n","  Downloading tsdownsample-0.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n","Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=2.0.4->pycaret) (3.0.11)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=2.0.4->pycaret) (2.2.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->pycaret) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->pycaret) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->pycaret) (2024.8.30)\n","Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (2.2.5)\n","Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (3.0.4)\n","Collecting dash-html-components==2.0.0 (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret)\n","  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n","Collecting dash-core-components==2.0.0 (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret)\n","  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting dash-table==5.0.0 (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret)\n","  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (4.12.2)\n","Collecting retrying (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret)\n","  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (1.6.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (6.1.12)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (6.3.3)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.5.0->pycaret) (0.8.4)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (0.20.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=4.2.0->pycaret) (4.3.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category-encoders>=2.4.0->pycaret) (1.16.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.5.0->pycaret) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->pycaret) (0.2.13)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (6.5.5)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (2.2.0)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (8.1.7)\n","Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (24.0.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (23.1.0)\n","Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (6.5.4)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.18.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.20.0)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.1.0)\n","Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.2.4)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (4.9.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (4.12.3)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (6.1.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.4)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.3.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.8.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.10.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.5.1)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.3.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (21.2.0)\n","Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.24.0)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.17.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (2.6)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.5.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (2.22)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (3.7.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.8.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.2.2)\n","Downloading pycaret-3.3.2-py3-none-any.whl (486 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.1/486.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading schemdraw-0.15-py3-none-any.whl (106 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sktime-0.26.0-py3-none-any.whl (21.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n","Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading plotly_resampler-0.10.0-py3-none-any.whl (80 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.7/80.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n","Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tbats-1.1.3-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n","Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dash-2.18.1-py3-none-any.whl (7.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n","Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n","Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n","Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_base-0.7.8-py3-none-any.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.1/130.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tsdownsample-0.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Building wheels for collected packages: pyod\n","  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyod: filename=pyod-2.0.2-py3-none-any.whl size=198469 sha256=a7fda545c2ddd2f4c897795929041f1365aca9ce6285c7ceb80ff8ec34c7f0b9\n","  Stored in directory: /root/.cache/pip/wheels/77/c2/20/34d1f15b41b701ba69f42a32304825810d680754d509f91391\n","Successfully built pyod\n","Installing collected packages: kaleido, dash-table, dash-html-components, dash-core-components, xxhash, wurlitzer, tsdownsample, scipy, scikit-base, schemdraw, retrying, orjson, joblib, jedi, deprecation, scikit-learn, sktime, scikit-plot, pyod, dash, pmdarima, plotly-resampler, category-encoders, tbats, pycaret\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.13.1\n","    Uninstalling scipy-1.13.1:\n","      Successfully uninstalled scipy-1.13.1\n","  Attempting uninstall: joblib\n","    Found existing installation: joblib 1.4.2\n","    Uninstalling joblib-1.4.2:\n","      Successfully uninstalled joblib-1.4.2\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.5.2\n","    Uninstalling scikit-learn-1.5.2:\n","      Successfully uninstalled scikit-learn-1.5.2\n","Successfully installed category-encoders-2.6.3 dash-2.18.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 deprecation-2.1.0 jedi-0.19.1 joblib-1.3.2 kaleido-0.2.1 orjson-3.10.7 plotly-resampler-0.10.0 pmdarima-2.0.4 pycaret-3.3.2 pyod-2.0.2 retrying-1.3.4 schemdraw-0.15 scikit-base-0.7.8 scikit-learn-1.4.2 scikit-plot-0.3.7 scipy-1.11.4 sktime-0.26.0 tbats-1.1.3 tsdownsample-0.1.3 wurlitzer-3.1.1 xxhash-3.5.0\n"]}],"source":["!pip install pycaret"]},{"cell_type":"markdown","id":"490d33a1","metadata":{"id":"490d33a1"},"source":["## 1. Installing and Importing PyCaret\n","PyCaret is a low-code machine learning library that allows us to perform model selection, training, and hyperparameter tuning efficiently. First, we need to install and import the necessary libraries."]},{"cell_type":"code","execution_count":2,"id":"a60d9331","metadata":{"id":"a60d9331","executionInfo":{"status":"ok","timestamp":1727113994247,"user_tz":-180,"elapsed":6227,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Importing necessary libraries\n","from pycaret.classification import *"]},{"cell_type":"markdown","id":"749bf25f","metadata":{"id":"749bf25f"},"source":["## 2. Setting Up the PyCaret Environment\n","We set up the PyCaret environment with the **diabetes dataset**. This will initialize the AutoML pipeline and allow us to train multiple models with minimal code."]},{"cell_type":"markdown","id":"0rbFvt3bHeUm","metadata":{"id":"0rbFvt3bHeUm"},"source":["**Load the preprocessed dataset**"]},{"cell_type":"code","execution_count":4,"id":"79bf95a8","metadata":{"id":"79bf95a8","executionInfo":{"status":"ok","timestamp":1727114027335,"user_tz":-180,"elapsed":355,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Load the preprocessed dataset\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","# Load the preprocessed diabetes data\n","data = pd.read_csv('preprocessed_diabetes_data.csv')"]},{"cell_type":"markdown","id":"v2WpdB88Hn2A","metadata":{"id":"v2WpdB88Hn2A"},"source":["**Performing Train test split**"]},{"cell_type":"code","execution_count":5,"id":"FGqprCkMHksk","metadata":{"id":"FGqprCkMHksk","executionInfo":{"status":"ok","timestamp":1727114035275,"user_tz":-180,"elapsed":315,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Split the data into training and testing sets\n","train_data, test_data = train_test_split(data, test_size=0.2, random_state=123)"]},{"cell_type":"markdown","id":"g6AVGRF7HuXm","metadata":{"id":"g6AVGRF7HuXm"},"source":["**PyCaret environment setup**\n","\n","We initialize the PyCaret environment for classification tasks using the `setup()` function. This step sets up the machine learning pipeline and prepares the dataset for training and evaluation.\n","\n","**Key Arguments**:\n","- `data=train_data`: We pass the training data for PyCaret to use in model training.\n","- `target='Outcome'`: The target variable is set to `Outcome`, which represents whether a patient has diabetes (1) or not (0).\n","- `session_id=123`: This argument ensures that the results are reproducible across runs by fixing the random seed.\n","- `normalize=True`: PyCaret automatically scales and normalizes the features to improve model performance.\n","- `fold=10`: We use 10-fold cross-validation to evaluate the performance of different models. This helps in producing more reliable and generalized results by splitting the data into 10 parts and evaluating each fold.\n","\n","This setup ensures that the data is preprocessed, and ready for model selection and tuning with PyCaret.\n"]},{"cell_type":"code","execution_count":6,"id":"bf41ed33","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":707},"id":"bf41ed33","outputId":"cac9f2de-5dca-4227-97a0-a3fcc9476233","executionInfo":{"status":"ok","timestamp":1727114202540,"user_tz":-180,"elapsed":2286,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7d3981382290>"],"text/html":["<style type=\"text/css\">\n","#T_bdeae_row8_col1, #T_bdeae_row12_col1 {\n","  background-color: lightgreen;\n","}\n","</style>\n","<table id=\"T_bdeae\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_bdeae_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n","      <th id=\"T_bdeae_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_bdeae_row0_col0\" class=\"data row0 col0\" >Session id</td>\n","      <td id=\"T_bdeae_row0_col1\" class=\"data row0 col1\" >123</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_bdeae_row1_col0\" class=\"data row1 col0\" >Target</td>\n","      <td id=\"T_bdeae_row1_col1\" class=\"data row1 col1\" >Outcome</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_bdeae_row2_col0\" class=\"data row2 col0\" >Target type</td>\n","      <td id=\"T_bdeae_row2_col1\" class=\"data row2 col1\" >Binary</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_bdeae_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n","      <td id=\"T_bdeae_row3_col1\" class=\"data row3 col1\" >(800, 9)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_bdeae_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n","      <td id=\"T_bdeae_row4_col1\" class=\"data row4 col1\" >(800, 9)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n","      <td id=\"T_bdeae_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n","      <td id=\"T_bdeae_row5_col1\" class=\"data row5 col1\" >(560, 9)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n","      <td id=\"T_bdeae_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n","      <td id=\"T_bdeae_row6_col1\" class=\"data row6 col1\" >(240, 9)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n","      <td id=\"T_bdeae_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n","      <td id=\"T_bdeae_row7_col1\" class=\"data row7 col1\" >8</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n","      <td id=\"T_bdeae_row8_col0\" class=\"data row8 col0\" >Preprocess</td>\n","      <td id=\"T_bdeae_row8_col1\" class=\"data row8 col1\" >True</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n","      <td id=\"T_bdeae_row9_col0\" class=\"data row9 col0\" >Imputation type</td>\n","      <td id=\"T_bdeae_row9_col1\" class=\"data row9 col1\" >simple</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n","      <td id=\"T_bdeae_row10_col0\" class=\"data row10 col0\" >Numeric imputation</td>\n","      <td id=\"T_bdeae_row10_col1\" class=\"data row10 col1\" >mean</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n","      <td id=\"T_bdeae_row11_col0\" class=\"data row11 col0\" >Categorical imputation</td>\n","      <td id=\"T_bdeae_row11_col1\" class=\"data row11 col1\" >mode</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n","      <td id=\"T_bdeae_row12_col0\" class=\"data row12 col0\" >Normalize</td>\n","      <td id=\"T_bdeae_row12_col1\" class=\"data row12 col1\" >True</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n","      <td id=\"T_bdeae_row13_col0\" class=\"data row13 col0\" >Normalize method</td>\n","      <td id=\"T_bdeae_row13_col1\" class=\"data row13 col1\" >zscore</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n","      <td id=\"T_bdeae_row14_col0\" class=\"data row14 col0\" >Fold Generator</td>\n","      <td id=\"T_bdeae_row14_col1\" class=\"data row14 col1\" >StratifiedKFold</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n","      <td id=\"T_bdeae_row15_col0\" class=\"data row15 col0\" >Fold Number</td>\n","      <td id=\"T_bdeae_row15_col1\" class=\"data row15 col1\" >10</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n","      <td id=\"T_bdeae_row16_col0\" class=\"data row16 col0\" >CPU Jobs</td>\n","      <td id=\"T_bdeae_row16_col1\" class=\"data row16 col1\" >-1</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n","      <td id=\"T_bdeae_row17_col0\" class=\"data row17 col0\" >Use GPU</td>\n","      <td id=\"T_bdeae_row17_col1\" class=\"data row17 col1\" >False</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n","      <td id=\"T_bdeae_row18_col0\" class=\"data row18 col0\" >Log Experiment</td>\n","      <td id=\"T_bdeae_row18_col1\" class=\"data row18 col1\" >False</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n","      <td id=\"T_bdeae_row19_col0\" class=\"data row19 col0\" >Experiment Name</td>\n","      <td id=\"T_bdeae_row19_col1\" class=\"data row19 col1\" >clf-default-name</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdeae_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n","      <td id=\"T_bdeae_row20_col0\" class=\"data row20 col0\" >USI</td>\n","      <td id=\"T_bdeae_row20_col1\" class=\"data row20 col1\" >0bf2</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{}}],"source":["# Initialize the PyCaret environment\n","clf = setup(train_data, target='Outcome', session_id=123, normalize=True, fold=10)"]},{"cell_type":"markdown","id":"EpxDRXmoI86l","metadata":{"id":"EpxDRXmoI86l"},"source":["### PyCaret Setup Output: Summary and Observations\n","\n","The following table summarizes the key configurations and transformations applied by PyCaret after initializing the `setup()` function for model selection and hyperparameter tuning:\n","\n","1. **Session ID**:  \n","   - **Value**: `123`  \n","   - This ensures reproducibility across different runs by fixing the random seed.\n","\n","2. **Target Variable**:  \n","   - **Value**: `Outcome`  \n","   - The `Outcome` variable represents whether a patient has diabetes (1) or not (0), making this a **binary classification** problem.\n","\n","3. **Target Type**:  \n","   - **Value**: `Binary`  \n","   - As the target variable has two possible outcomes, the classification task is recognized as binary.\n","\n","4. **Original Data Shape**:  \n","   - **Value**: `(800, 9)`  \n","   - The dataset contains 800 rows and 9 columns after splitting the data into training and testing sets.\n","\n","5. **Transformed Data Shape**:  \n","   - **Value**: `(800, 9)`  \n","   - After preprocessing, the data maintains the same dimensions, indicating no columns were dropped or added during transformations.\n","\n","6. **Transformed Train Set Shape**:  \n","   - **Value**: `(560, 9)`  \n","   - The training set consists of 560 rows and 9 columns, following an 80-20 split of the dataset.\n","\n","7. **Transformed Test Set Shape**:  \n","   - **Value**: `(240, 9)`  \n","   - The testing set consists of 240 rows and 9 columns.\n","\n","8. **Numeric Features**:  \n","   - **Value**: `8`  \n","   - The dataset contains 8 numeric features, all of which are continuous variables.\n","\n","9. **Preprocessing Applied**:  \n","   - **Value**: `True`  \n","   - PyCaret applied preprocessing steps such as imputation and normalization.\n","\n","10. **Imputation Type**:  \n","   - **Numeric Imputation**: `mean`  \n","   - **Categorical Imputation**: `mode`  \n","   - Numeric features with missing values were imputed using the mean, and categorical features (if any) would have been imputed using the mode (though none exist in this dataset).\n","\n","11. **Normalization**:  \n","   - **Value**: `True`  \n","   - Normalization has been applied to the numeric features to ensure they are on the same scale.\n","\n","12. **Normalization Method**:  \n","   - **Value**: `zscore`  \n","   - The normalization method used is **Z-score normalization**, which standardizes the data by removing the mean and scaling to unit variance.\n","\n","13. **Cross-Validation Method**:  \n","   - **Fold Generator**: `StratifiedKFold`  \n","   - **Fold Number**: `10`  \n","   - A 10-fold **stratified cross-validation** method has been applied, ensuring the distribution of the target variable is preserved in each fold.\n","\n","14. **CPU Jobs**:  \n","   - **Value**: `-1`  \n","   - PyCaret utilizes all available CPU cores to parallelize computations, speeding up the model training process.\n","\n","15. **GPU Usage**:  \n","   - **Value**: `False`  \n","   - No GPU was used for computations, as this configuration defaults to CPU-only operations.\n","\n","16. **Log Experiment**:  \n","   - **Value**: `False`  \n","   - Experiment logging is disabled by default, though it can be enabled to track results for deeper analysis.\n","\n","17. **Experiment Name**:  \n","   - **Value**: `clf-default-name`  \n","   - PyCaret has assigned a default experiment name for this classification task.\n","\n","18. **Unique Session ID (USI)**:  \n","   - **Value**: `6bcf`  \n","   - A unique session ID is assigned to this experiment for tracking purposes.\n","\n","### Conclusion:\n","The PyCaret environment has been successfully initialized with appropriate preprocessing steps and a 10-fold stratified cross-validation setup. The dataset is now prepared for training and tuning multiple models, allowing for efficient model selection based on performance metrics.\n"]},{"cell_type":"markdown","id":"73dbb078","metadata":{"id":"73dbb078"},"source":["## 3. Comparing and Selecting Models\n","PyCaret allows us to train and compare several machine learning models automatically. We will use the **compare_models** function to identify the top-performing models based on accuracy.\n","\n","After initializing the PyCaret environment, we proceed to the **model comparison** stage. In this step, PyCaret evaluates multiple classification models and compares their performance based on cross-validation results.\n","\n","The `compare_models()` function automatically trains and evaluates a wide range of machine learning models using the specified number of folds (in this case, 10-fold cross-validation). It then ranks the models based on a default performance metric, typically **Accuracy** for classification tasks.\n","\n","- **n_select=3**: This argument specifies that we want to select the top 3 best-performing models based on their cross-validation scores.\n"]},{"cell_type":"code","execution_count":7,"id":"a302f603","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519,"referenced_widgets":["3b5afc132bde4190abc48013f9c5a691","9629f4ea62fe4657b776ecbbbfb9a165","b0553365ed47429db0e2cdefe52291fc","7a035e1598fb44a798229a4ec8a93e4f","8dc88b10607b4b058904ed8b76717ae9","9a28dc99d7034872897151310e8b4caa","2c132885e13d4f698389953be3ebd4c2","cb87e5e6c00f463f9926f9110217677b","517284e2be994e9eae01c23852d77564","dd037e392873436aa9c648c48c57da7e","51f00efe493b4fdf832ad0a16a96123d"]},"id":"a302f603","outputId":"0f54f8d2-bb31-4ac9-a55a-eeb213a7bb49","executionInfo":{"status":"ok","timestamp":1727114241288,"user_tz":-180,"elapsed":32530,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7d39815bda20>"],"text/html":["<style type=\"text/css\">\n","#T_cd9a0 th {\n","  text-align: left;\n","}\n","#T_cd9a0_row0_col0, #T_cd9a0_row1_col0, #T_cd9a0_row1_col1, #T_cd9a0_row1_col2, #T_cd9a0_row1_col3, #T_cd9a0_row1_col4, #T_cd9a0_row1_col5, #T_cd9a0_row1_col6, #T_cd9a0_row1_col7, #T_cd9a0_row2_col0, #T_cd9a0_row2_col1, #T_cd9a0_row2_col2, #T_cd9a0_row2_col3, #T_cd9a0_row2_col4, #T_cd9a0_row2_col5, #T_cd9a0_row2_col6, #T_cd9a0_row2_col7, #T_cd9a0_row3_col0, #T_cd9a0_row3_col1, #T_cd9a0_row3_col2, #T_cd9a0_row3_col3, #T_cd9a0_row3_col4, #T_cd9a0_row3_col5, #T_cd9a0_row3_col6, #T_cd9a0_row3_col7, #T_cd9a0_row4_col0, #T_cd9a0_row4_col1, #T_cd9a0_row4_col2, #T_cd9a0_row4_col3, #T_cd9a0_row4_col4, #T_cd9a0_row4_col5, #T_cd9a0_row4_col6, #T_cd9a0_row4_col7, #T_cd9a0_row5_col0, #T_cd9a0_row5_col1, #T_cd9a0_row5_col2, #T_cd9a0_row5_col3, #T_cd9a0_row5_col4, #T_cd9a0_row5_col5, #T_cd9a0_row5_col6, #T_cd9a0_row5_col7, #T_cd9a0_row6_col0, #T_cd9a0_row6_col1, #T_cd9a0_row6_col2, #T_cd9a0_row6_col3, #T_cd9a0_row6_col4, #T_cd9a0_row6_col5, #T_cd9a0_row6_col6, #T_cd9a0_row6_col7, #T_cd9a0_row7_col0, #T_cd9a0_row7_col1, #T_cd9a0_row7_col2, #T_cd9a0_row7_col3, #T_cd9a0_row7_col4, #T_cd9a0_row7_col5, #T_cd9a0_row7_col6, #T_cd9a0_row7_col7, #T_cd9a0_row8_col0, #T_cd9a0_row8_col1, #T_cd9a0_row8_col2, #T_cd9a0_row8_col3, #T_cd9a0_row8_col4, #T_cd9a0_row8_col5, #T_cd9a0_row8_col6, #T_cd9a0_row8_col7, #T_cd9a0_row9_col0, #T_cd9a0_row9_col1, #T_cd9a0_row9_col2, #T_cd9a0_row9_col3, #T_cd9a0_row9_col4, #T_cd9a0_row9_col5, #T_cd9a0_row9_col6, #T_cd9a0_row9_col7, #T_cd9a0_row10_col0, #T_cd9a0_row10_col1, #T_cd9a0_row10_col2, #T_cd9a0_row10_col3, #T_cd9a0_row10_col4, #T_cd9a0_row10_col5, #T_cd9a0_row10_col6, #T_cd9a0_row10_col7, #T_cd9a0_row11_col0, #T_cd9a0_row11_col1, #T_cd9a0_row11_col2, #T_cd9a0_row11_col3, #T_cd9a0_row11_col4, #T_cd9a0_row11_col5, #T_cd9a0_row11_col6, #T_cd9a0_row11_col7, #T_cd9a0_row12_col0, #T_cd9a0_row12_col1, #T_cd9a0_row12_col2, #T_cd9a0_row12_col3, #T_cd9a0_row12_col4, #T_cd9a0_row12_col5, #T_cd9a0_row12_col6, #T_cd9a0_row12_col7, #T_cd9a0_row13_col0, #T_cd9a0_row13_col1, #T_cd9a0_row13_col2, #T_cd9a0_row13_col3, #T_cd9a0_row13_col4, #T_cd9a0_row13_col5, #T_cd9a0_row13_col6, #T_cd9a0_row13_col7, #T_cd9a0_row14_col0, #T_cd9a0_row14_col1, #T_cd9a0_row14_col2, #T_cd9a0_row14_col3, #T_cd9a0_row14_col4, #T_cd9a0_row14_col5, #T_cd9a0_row14_col6, #T_cd9a0_row14_col7 {\n","  text-align: left;\n","}\n","#T_cd9a0_row0_col1, #T_cd9a0_row0_col2, #T_cd9a0_row0_col3, #T_cd9a0_row0_col4, #T_cd9a0_row0_col5, #T_cd9a0_row0_col6, #T_cd9a0_row0_col7 {\n","  text-align: left;\n","  background-color: yellow;\n","}\n","#T_cd9a0_row0_col8, #T_cd9a0_row1_col8, #T_cd9a0_row2_col8, #T_cd9a0_row3_col8, #T_cd9a0_row4_col8, #T_cd9a0_row5_col8, #T_cd9a0_row6_col8, #T_cd9a0_row7_col8, #T_cd9a0_row8_col8, #T_cd9a0_row9_col8, #T_cd9a0_row10_col8, #T_cd9a0_row11_col8, #T_cd9a0_row12_col8, #T_cd9a0_row13_col8 {\n","  text-align: left;\n","  background-color: lightgrey;\n","}\n","#T_cd9a0_row14_col8 {\n","  text-align: left;\n","  background-color: yellow;\n","  background-color: lightgrey;\n","}\n","</style>\n","<table id=\"T_cd9a0\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_cd9a0_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n","      <th id=\"T_cd9a0_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n","      <th id=\"T_cd9a0_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n","      <th id=\"T_cd9a0_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n","      <th id=\"T_cd9a0_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n","      <th id=\"T_cd9a0_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n","      <th id=\"T_cd9a0_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n","      <th id=\"T_cd9a0_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n","      <th id=\"T_cd9a0_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row0\" class=\"row_heading level0 row0\" >et</th>\n","      <td id=\"T_cd9a0_row0_col0\" class=\"data row0 col0\" >Extra Trees Classifier</td>\n","      <td id=\"T_cd9a0_row0_col1\" class=\"data row0 col1\" >0.8089</td>\n","      <td id=\"T_cd9a0_row0_col2\" class=\"data row0 col2\" >0.8957</td>\n","      <td id=\"T_cd9a0_row0_col3\" class=\"data row0 col3\" >0.8241</td>\n","      <td id=\"T_cd9a0_row0_col4\" class=\"data row0 col4\" >0.8072</td>\n","      <td id=\"T_cd9a0_row0_col5\" class=\"data row0 col5\" >0.8110</td>\n","      <td id=\"T_cd9a0_row0_col6\" class=\"data row0 col6\" >0.6176</td>\n","      <td id=\"T_cd9a0_row0_col7\" class=\"data row0 col7\" >0.6242</td>\n","      <td id=\"T_cd9a0_row0_col8\" class=\"data row0 col8\" >0.1870</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row1\" class=\"row_heading level0 row1\" >xgboost</th>\n","      <td id=\"T_cd9a0_row1_col0\" class=\"data row1 col0\" >Extreme Gradient Boosting</td>\n","      <td id=\"T_cd9a0_row1_col1\" class=\"data row1 col1\" >0.7857</td>\n","      <td id=\"T_cd9a0_row1_col2\" class=\"data row1 col2\" >0.8598</td>\n","      <td id=\"T_cd9a0_row1_col3\" class=\"data row1 col3\" >0.8136</td>\n","      <td id=\"T_cd9a0_row1_col4\" class=\"data row1 col4\" >0.7750</td>\n","      <td id=\"T_cd9a0_row1_col5\" class=\"data row1 col5\" >0.7902</td>\n","      <td id=\"T_cd9a0_row1_col6\" class=\"data row1 col6\" >0.5714</td>\n","      <td id=\"T_cd9a0_row1_col7\" class=\"data row1 col7\" >0.5786</td>\n","      <td id=\"T_cd9a0_row1_col8\" class=\"data row1 col8\" >0.0980</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row2\" class=\"row_heading level0 row2\" >rf</th>\n","      <td id=\"T_cd9a0_row2_col0\" class=\"data row2 col0\" >Random Forest Classifier</td>\n","      <td id=\"T_cd9a0_row2_col1\" class=\"data row2 col1\" >0.7839</td>\n","      <td id=\"T_cd9a0_row2_col2\" class=\"data row2 col2\" >0.8777</td>\n","      <td id=\"T_cd9a0_row2_col3\" class=\"data row2 col3\" >0.7992</td>\n","      <td id=\"T_cd9a0_row2_col4\" class=\"data row2 col4\" >0.7828</td>\n","      <td id=\"T_cd9a0_row2_col5\" class=\"data row2 col5\" >0.7874</td>\n","      <td id=\"T_cd9a0_row2_col6\" class=\"data row2 col6\" >0.5677</td>\n","      <td id=\"T_cd9a0_row2_col7\" class=\"data row2 col7\" >0.5731</td>\n","      <td id=\"T_cd9a0_row2_col8\" class=\"data row2 col8\" >0.3790</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row3\" class=\"row_heading level0 row3\" >gbc</th>\n","      <td id=\"T_cd9a0_row3_col0\" class=\"data row3 col0\" >Gradient Boosting Classifier</td>\n","      <td id=\"T_cd9a0_row3_col1\" class=\"data row3 col1\" >0.7821</td>\n","      <td id=\"T_cd9a0_row3_col2\" class=\"data row3 col2\" >0.8701</td>\n","      <td id=\"T_cd9a0_row3_col3\" class=\"data row3 col3\" >0.7849</td>\n","      <td id=\"T_cd9a0_row3_col4\" class=\"data row3 col4\" >0.7851</td>\n","      <td id=\"T_cd9a0_row3_col5\" class=\"data row3 col5\" >0.7817</td>\n","      <td id=\"T_cd9a0_row3_col6\" class=\"data row3 col6\" >0.5641</td>\n","      <td id=\"T_cd9a0_row3_col7\" class=\"data row3 col7\" >0.5684</td>\n","      <td id=\"T_cd9a0_row3_col8\" class=\"data row3 col8\" >0.1740</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row4\" class=\"row_heading level0 row4\" >lightgbm</th>\n","      <td id=\"T_cd9a0_row4_col0\" class=\"data row4 col0\" >Light Gradient Boosting Machine</td>\n","      <td id=\"T_cd9a0_row4_col1\" class=\"data row4 col1\" >0.7821</td>\n","      <td id=\"T_cd9a0_row4_col2\" class=\"data row4 col2\" >0.8682</td>\n","      <td id=\"T_cd9a0_row4_col3\" class=\"data row4 col3\" >0.7954</td>\n","      <td id=\"T_cd9a0_row4_col4\" class=\"data row4 col4\" >0.7797</td>\n","      <td id=\"T_cd9a0_row4_col5\" class=\"data row4 col5\" >0.7833</td>\n","      <td id=\"T_cd9a0_row4_col6\" class=\"data row4 col6\" >0.5640</td>\n","      <td id=\"T_cd9a0_row4_col7\" class=\"data row4 col7\" >0.5709</td>\n","      <td id=\"T_cd9a0_row4_col8\" class=\"data row4 col8\" >1.0300</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row5\" class=\"row_heading level0 row5\" >ada</th>\n","      <td id=\"T_cd9a0_row5_col0\" class=\"data row5 col0\" >Ada Boost Classifier</td>\n","      <td id=\"T_cd9a0_row5_col1\" class=\"data row5 col1\" >0.7696</td>\n","      <td id=\"T_cd9a0_row5_col2\" class=\"data row5 col2\" >0.8460</td>\n","      <td id=\"T_cd9a0_row5_col3\" class=\"data row5 col3\" >0.7958</td>\n","      <td id=\"T_cd9a0_row5_col4\" class=\"data row5 col4\" >0.7622</td>\n","      <td id=\"T_cd9a0_row5_col5\" class=\"data row5 col5\" >0.7756</td>\n","      <td id=\"T_cd9a0_row5_col6\" class=\"data row5 col6\" >0.5392</td>\n","      <td id=\"T_cd9a0_row5_col7\" class=\"data row5 col7\" >0.5443</td>\n","      <td id=\"T_cd9a0_row5_col8\" class=\"data row5 col8\" >0.1270</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row6\" class=\"row_heading level0 row6\" >knn</th>\n","      <td id=\"T_cd9a0_row6_col0\" class=\"data row6 col0\" >K Neighbors Classifier</td>\n","      <td id=\"T_cd9a0_row6_col1\" class=\"data row6 col1\" >0.7607</td>\n","      <td id=\"T_cd9a0_row6_col2\" class=\"data row6 col2\" >0.8257</td>\n","      <td id=\"T_cd9a0_row6_col3\" class=\"data row6 col3\" >0.7915</td>\n","      <td id=\"T_cd9a0_row6_col4\" class=\"data row6 col4\" >0.7464</td>\n","      <td id=\"T_cd9a0_row6_col5\" class=\"data row6 col5\" >0.7637</td>\n","      <td id=\"T_cd9a0_row6_col6\" class=\"data row6 col6\" >0.5211</td>\n","      <td id=\"T_cd9a0_row6_col7\" class=\"data row6 col7\" >0.5291</td>\n","      <td id=\"T_cd9a0_row6_col8\" class=\"data row6 col8\" >0.0500</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row7\" class=\"row_heading level0 row7\" >ridge</th>\n","      <td id=\"T_cd9a0_row7_col0\" class=\"data row7 col0\" >Ridge Classifier</td>\n","      <td id=\"T_cd9a0_row7_col1\" class=\"data row7 col1\" >0.7446</td>\n","      <td id=\"T_cd9a0_row7_col2\" class=\"data row7 col2\" >0.8328</td>\n","      <td id=\"T_cd9a0_row7_col3\" class=\"data row7 col3\" >0.7202</td>\n","      <td id=\"T_cd9a0_row7_col4\" class=\"data row7 col4\" >0.7703</td>\n","      <td id=\"T_cd9a0_row7_col5\" class=\"data row7 col5\" >0.7368</td>\n","      <td id=\"T_cd9a0_row7_col6\" class=\"data row7 col6\" >0.4890</td>\n","      <td id=\"T_cd9a0_row7_col7\" class=\"data row7 col7\" >0.4990</td>\n","      <td id=\"T_cd9a0_row7_col8\" class=\"data row7 col8\" >0.0340</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row8\" class=\"row_heading level0 row8\" >lda</th>\n","      <td id=\"T_cd9a0_row8_col0\" class=\"data row8 col0\" >Linear Discriminant Analysis</td>\n","      <td id=\"T_cd9a0_row8_col1\" class=\"data row8 col1\" >0.7446</td>\n","      <td id=\"T_cd9a0_row8_col2\" class=\"data row8 col2\" >0.8326</td>\n","      <td id=\"T_cd9a0_row8_col3\" class=\"data row8 col3\" >0.7202</td>\n","      <td id=\"T_cd9a0_row8_col4\" class=\"data row8 col4\" >0.7703</td>\n","      <td id=\"T_cd9a0_row8_col5\" class=\"data row8 col5\" >0.7368</td>\n","      <td id=\"T_cd9a0_row8_col6\" class=\"data row8 col6\" >0.4890</td>\n","      <td id=\"T_cd9a0_row8_col7\" class=\"data row8 col7\" >0.4990</td>\n","      <td id=\"T_cd9a0_row8_col8\" class=\"data row8 col8\" >0.0310</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row9\" class=\"row_heading level0 row9\" >lr</th>\n","      <td id=\"T_cd9a0_row9_col0\" class=\"data row9 col0\" >Logistic Regression</td>\n","      <td id=\"T_cd9a0_row9_col1\" class=\"data row9 col1\" >0.7429</td>\n","      <td id=\"T_cd9a0_row9_col2\" class=\"data row9 col2\" >0.8328</td>\n","      <td id=\"T_cd9a0_row9_col3\" class=\"data row9 col3\" >0.7311</td>\n","      <td id=\"T_cd9a0_row9_col4\" class=\"data row9 col4\" >0.7616</td>\n","      <td id=\"T_cd9a0_row9_col5\" class=\"data row9 col5\" >0.7393</td>\n","      <td id=\"T_cd9a0_row9_col6\" class=\"data row9 col6\" >0.4855</td>\n","      <td id=\"T_cd9a0_row9_col7\" class=\"data row9 col7\" >0.4939</td>\n","      <td id=\"T_cd9a0_row9_col8\" class=\"data row9 col8\" >0.4710</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row10\" class=\"row_heading level0 row10\" >qda</th>\n","      <td id=\"T_cd9a0_row10_col0\" class=\"data row10 col0\" >Quadratic Discriminant Analysis</td>\n","      <td id=\"T_cd9a0_row10_col1\" class=\"data row10 col1\" >0.7411</td>\n","      <td id=\"T_cd9a0_row10_col2\" class=\"data row10 col2\" >0.8202</td>\n","      <td id=\"T_cd9a0_row10_col3\" class=\"data row10 col3\" >0.7164</td>\n","      <td id=\"T_cd9a0_row10_col4\" class=\"data row10 col4\" >0.7578</td>\n","      <td id=\"T_cd9a0_row10_col5\" class=\"data row10 col5\" >0.7302</td>\n","      <td id=\"T_cd9a0_row10_col6\" class=\"data row10 col6\" >0.4817</td>\n","      <td id=\"T_cd9a0_row10_col7\" class=\"data row10 col7\" >0.4895</td>\n","      <td id=\"T_cd9a0_row10_col8\" class=\"data row10 col8\" >0.0530</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row11\" class=\"row_heading level0 row11\" >nb</th>\n","      <td id=\"T_cd9a0_row11_col0\" class=\"data row11 col0\" >Naive Bayes</td>\n","      <td id=\"T_cd9a0_row11_col1\" class=\"data row11 col1\" >0.7161</td>\n","      <td id=\"T_cd9a0_row11_col2\" class=\"data row11 col2\" >0.8074</td>\n","      <td id=\"T_cd9a0_row11_col3\" class=\"data row11 col3\" >0.6878</td>\n","      <td id=\"T_cd9a0_row11_col4\" class=\"data row11 col4\" >0.7345</td>\n","      <td id=\"T_cd9a0_row11_col5\" class=\"data row11 col5\" >0.7063</td>\n","      <td id=\"T_cd9a0_row11_col6\" class=\"data row11 col6\" >0.4318</td>\n","      <td id=\"T_cd9a0_row11_col7\" class=\"data row11 col7\" >0.4364</td>\n","      <td id=\"T_cd9a0_row11_col8\" class=\"data row11 col8\" >0.0330</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row12\" class=\"row_heading level0 row12\" >svm</th>\n","      <td id=\"T_cd9a0_row12_col0\" class=\"data row12 col0\" >SVM - Linear Kernel</td>\n","      <td id=\"T_cd9a0_row12_col1\" class=\"data row12 col1\" >0.6929</td>\n","      <td id=\"T_cd9a0_row12_col2\" class=\"data row12 col2\" >0.7532</td>\n","      <td id=\"T_cd9a0_row12_col3\" class=\"data row12 col3\" >0.6452</td>\n","      <td id=\"T_cd9a0_row12_col4\" class=\"data row12 col4\" >0.7244</td>\n","      <td id=\"T_cd9a0_row12_col5\" class=\"data row12 col5\" >0.6730</td>\n","      <td id=\"T_cd9a0_row12_col6\" class=\"data row12 col6\" >0.3855</td>\n","      <td id=\"T_cd9a0_row12_col7\" class=\"data row12 col7\" >0.3951</td>\n","      <td id=\"T_cd9a0_row12_col8\" class=\"data row12 col8\" >0.0380</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row13\" class=\"row_heading level0 row13\" >dt</th>\n","      <td id=\"T_cd9a0_row13_col0\" class=\"data row13 col0\" >Decision Tree Classifier</td>\n","      <td id=\"T_cd9a0_row13_col1\" class=\"data row13 col1\" >0.6839</td>\n","      <td id=\"T_cd9a0_row13_col2\" class=\"data row13 col2\" >0.6839</td>\n","      <td id=\"T_cd9a0_row13_col3\" class=\"data row13 col3\" >0.6847</td>\n","      <td id=\"T_cd9a0_row13_col4\" class=\"data row13 col4\" >0.6886</td>\n","      <td id=\"T_cd9a0_row13_col5\" class=\"data row13 col5\" >0.6823</td>\n","      <td id=\"T_cd9a0_row13_col6\" class=\"data row13 col6\" >0.3678</td>\n","      <td id=\"T_cd9a0_row13_col7\" class=\"data row13 col7\" >0.3731</td>\n","      <td id=\"T_cd9a0_row13_col8\" class=\"data row13 col8\" >0.0350</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cd9a0_level0_row14\" class=\"row_heading level0 row14\" >dummy</th>\n","      <td id=\"T_cd9a0_row14_col0\" class=\"data row14 col0\" >Dummy Classifier</td>\n","      <td id=\"T_cd9a0_row14_col1\" class=\"data row14 col1\" >0.5018</td>\n","      <td id=\"T_cd9a0_row14_col2\" class=\"data row14 col2\" >0.5000</td>\n","      <td id=\"T_cd9a0_row14_col3\" class=\"data row14 col3\" >0.0000</td>\n","      <td id=\"T_cd9a0_row14_col4\" class=\"data row14 col4\" >0.0000</td>\n","      <td id=\"T_cd9a0_row14_col5\" class=\"data row14 col5\" >0.0000</td>\n","      <td id=\"T_cd9a0_row14_col6\" class=\"data row14 col6\" >0.0000</td>\n","      <td id=\"T_cd9a0_row14_col7\" class=\"data row14 col7\" >0.0000</td>\n","      <td id=\"T_cd9a0_row14_col8\" class=\"data row14 col8\" >0.0290</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Processing:   0%|          | 0/67 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b5afc132bde4190abc48013f9c5a691"}},"metadata":{"application/vnd.jupyter.widget-view+json":{"colab":{"custom_widget_manager":{"url":"https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"}}}}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}],"source":["# Compare various models and select the top 3 best-performing ones\n","# PyCaret evaluates multiple models based on cross-validation and ranks them\n","# Compare models with progress bar and real-time output\n","best_model = compare_models(n_select=3, verbose=True)"]},{"cell_type":"markdown","id":"h386T2EaMCEz","metadata":{"id":"h386T2EaMCEz"},"source":["### Model Comparison Results: Observations and Analysis\n","\n","The table above presents the performance of multiple machine learning models evaluated using **10-fold cross-validation**. Various metrics such as **Accuracy**, **AUC**, **Recall**, **Precision**, **F1-score**, **Kappa**, **MCC** (Matthews Correlation Coefficient), and **TT (Training Time in seconds)** are compared to determine the best-performing model.\n","\n","#### Key Metrics:\n","- **Accuracy**: The proportion of correctly predicted instances out of the total instances.\n","- **AUC (Area Under the Curve)**: Measures the ability of the model to distinguish between classes. A higher AUC indicates better model performance.\n","- **Recall**: The proportion of actual positives that were identified correctly.\n","- **Precision**: The proportion of predicted positives that are actually positive.\n","- **F1-Score**: The harmonic mean of Precision and Recall. It balances the two metrics and is especially useful when the class distribution is uneven.\n","- **Kappa**: Measures the agreement between predicted and actual values, adjusted for the possibility of agreement occurring by chance.\n","- **MCC (Matthews Correlation Coefficient)**: A balanced measure for binary classification problems, accounting for true and false positives and negatives.\n","- **TT (Training Time)**: The time taken to train the model in seconds.\n","\n","---\n","\n","#### **1. Best-Performing Model: Extra Trees Classifier (et)**\n","- **Accuracy**: `0.8089` (Highest in the table)\n","- **AUC**: `0.8957` (Highest AUC, indicating excellent ability to distinguish between classes)\n","- **Recall**: `0.8241` (High recall, meaning the model correctly identifies a high proportion of actual positives)\n","- **Precision**: `0.8072` (Balanced precision, indicating a good proportion of predicted positives are correct)\n","- **F1-Score**: `0.8110` (The highest F1-score, balancing Precision and Recall effectively)\n","- **Kappa**: `0.6176` (Shows a good level of agreement between predictions and actual outcomes)\n","- **MCC**: `0.6242` (Indicates strong overall predictive performance)\n","\n","**Conclusion**:  \n","The **Extra Trees Classifier (et)** outperforms all other models in terms of Accuracy, AUC, Recall, Precision, and F1-Score. This model is the best-performing one overall and offers a balance of high predictive performance and robustness in distinguishing between classes.\n","\n","---\n","\n","#### **2. Second Best: Extreme Gradient Boosting (xgboost)**\n","- **Accuracy**: `0.7857` (Close to the top performer)\n","- **AUC**: `0.8598` (Slightly lower than Extra Trees, but still strong in terms of distinguishing classes)\n","- **Recall**: `0.8136` (High recall, but slightly lower than Extra Trees)\n","- **Precision**: `0.7750` (Lower than Extra Trees, leading to a lower F1-score)\n","- **F1-Score**: `0.7902`\n","- **Kappa**: `0.5714`  \n","- **MCC**: `0.5786`\n","\n","**Conclusion**:  \n","The **Extreme Gradient Boosting (xgboost)** model is a close second in performance, with good recall and AUC. However, its lower precision results in a slightly lower F1-score compared to Extra Trees.\n","\n","---\n","\n","#### **3. Random Forest Classifier (rf)**\n","- **Accuracy**: `0.7839`\n","- **AUC**: `0.8777`\n","- **Recall**: `0.7992`\n","- **Precision**: `0.7828`\n","- **F1-Score**: `0.7874`\n","- **Kappa**: `0.5677`\n","- **MCC**: `0.5731`\n","\n","**Conclusion**:  \n","The **Random Forest Classifier (rf)** also performs well, with good AUC, precision, and recall. While slightly behind the Extra Trees and XGBoost models, it is a strong contender and a popular choice for classification tasks.\n","\n","---\n","\n","#### **4. Training Time (TT) Analysis**\n","- **Fastest Training Time**: `SVM - Linear Kernel` (`0.0390 seconds`)\n","- **Slowest Training Time**: `Logistic Regression` (`0.6960 seconds`)\n","- **Extra Trees Training Time**: `0.4060 seconds`\n","\n","**Conclusion**:  \n","The Extra Trees model, while the best in terms of performance metrics, takes slightly longer to train (`0.4060 seconds`) compared to other models like `K Neighbors Classifier` (`0.1040 seconds`) or `Naive Bayes` (`0.0870 seconds`). However, the trade-off in time is reasonable given its significantly better performance.\n","\n","---\n","\n","#### **5. Other Models**\n","- **K Neighbors Classifier (knn)**:  \n","  - Accuracy of `0.7607`, with a good AUC (`0.8257`) and recall (`0.7915`), but lower precision and F1-score compared to the top-performing models.\n","  - **Observation**: This model has a relatively low training time (`0.1040 seconds`), making it a good choice for quick model deployment with moderate performance.\n","  \n","- **LightGBM**:  \n","  - Accuracy of `0.7821`, with a good AUC (`0.8682`) and F1-Score (`0.7833`).\n","  - **Observation**: A strong model with a balance between speed and performance, slightly behind the top three models.\n","\n","---\n","\n","### Final Analysis and Model Selection\n","The **Extra Trees Classifier** emerges as the best model across all key metrics such as Accuracy, AUC, Precision, Recall, and F1-score. It provides a good balance of predictive performance, class separation, and robustness. While it takes slightly longer to train, the trade-off is worth the improved model performance."]},{"cell_type":"code","execution_count":null,"id":"tM4uMrnyJ3lz","metadata":{"id":"tM4uMrnyJ3lz","executionInfo":{"status":"aborted","timestamp":1727113995037,"user_tz":-180,"elapsed":28,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Display the details of the selected models\n","best_model"]},{"cell_type":"markdown","id":"UwwgVae5Mvou","metadata":{"id":"UwwgVae5Mvou"},"source":["### Analyzing the Selected Models\n","\n","The `compare_models()` function has returned the top 3 best-performing models based on the cross-validation metrics. Below is a detailed analysis of the selected models, their parameters, and configurations:\n","\n","#### **1. Extra Trees Classifier (`ExtraTreesClassifier`)**\n","- **Bootstrap**: `False` – Bootstrap sampling was not used in this model.\n","- **Criterion**: `gini` – The Gini impurity was used to measure the quality of splits.\n","- **Max Features**: `sqrt` – A square root of the number of features was considered when looking for the best split.\n","- **Number of Estimators**: `100` – The model used 100 trees (default) in the forest.\n","- **n_jobs**: `-1` – All CPU cores were utilized to parallelize the computation.\n","- **Random State**: `123` – Ensures reproducibility of results.\n","\n","**Observation**:  \n","The **Extra Trees Classifier** is the top-performing model and it uses a Gini criterion and parallelized computation for training. With 100 trees in the ensemble and no bootstrapping, it was able to achieve the best performance across multiple metrics.\n","\n","---\n","\n","#### **2. XGBoost Classifier (`XGBClassifier`)**\n","- **Booster**: `'gbtree'` – This model used tree-based boosting.\n","- **Objective**: `'binary:logistic'` – The objective function used was logistic regression for binary classification.\n","- **Device**: `'cpu'` – Computation was done on the CPU.\n","- **n_jobs**: `-1` – The model used all available CPU cores for parallel computation.\n","- **Random State**: `123` – Ensures reproducibility of results.\n","\n","**Observation**:  \n","The **XGBoost Classifier** is known for its performance in handling large datasets and complex models. It uses tree boosting to minimize errors, making it a close second in terms of overall performance.\n","\n","---\n","\n","#### **3. Random Forest Classifier (`RandomForestClassifier`)**\n","- **Bootstrap**: `True` – Bootstrap sampling was used in this model.\n","- **Criterion**: `gini` – The Gini impurity was used to measure the quality of splits.\n","- **Max Features**: `sqrt` – A square root of the number of features was considered when looking for the best split.\n","- **Number of Estimators**: `100` – The model used 100 trees in the forest.\n","- **n_jobs**: `-1` – The model used all CPU cores to speed up the computation.\n","- **Random State**: `123` – Ensures reproducibility of results.\n","\n","**Observation**:  \n","The **Random Forest Classifier** uses bootstrapping and random feature selection to build a forest of decision trees. While similar in structure to the Extra Trees Classifier, it performs slightly worse but remains a strong contender due to its robust ensemble approach.\n","\n","---\n","\n","### Conclusion\n","\n","The top three models selected by PyCaret—**Extra Trees Classifier**, **XGBoost Classifier**, and **Random Forest Classifier**—are all ensemble models. These models work by combining multiple weak learners (trees) to produce a strong predictive model. Each of these models has been configured to run efficiently using all available CPU cores (`n_jobs=-1`) and has been set up with reproducible results (`random_state=123`).\n","\n","- **Extra Trees Classifier** is the best performer across the board, thanks to its ability to decorrelate trees and avoid overfitting.\n","- **XGBoost Classifier** is highly efficient and excels in handling complex datasets.\n","- **Random Forest Classifier** is a classic ensemble method known for its robustness and ease of tuning.\n","\n","In the next step, we will explore hyperparameter tuning for these models to further optimize their performance.\n"]},{"cell_type":"markdown","id":"3cdd7264","metadata":{"id":"3cdd7264"},"source":["## 4. Hyperparameter Tuning\n","\n","In this step, we will tune the hyperparameters of the top 3 models selected earlier: **Extra Trees Classifier**, **XGBoost Classifier**, and **Random Forest Classifier**. The tuning process involves automatically searching for the best hyperparameter combinations to optimize the performance of each model.\n","\n","After tuning, we will compare the results and select the best model for use in the final stage.\n"]},{"cell_type":"markdown","id":"FsmEb54uOTFf","metadata":{"id":"FsmEb54uOTFf"},"source":["**Tuning the top 3 models with verbose output**"]},{"cell_type":"code","execution_count":null,"id":"930d0ca4","metadata":{"id":"930d0ca4","executionInfo":{"status":"aborted","timestamp":1727113995037,"user_tz":-180,"elapsed":28,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Tune Extra Trees Classifier\n","tuned_et = tune_model(best_model[0], verbose=True)"]},{"cell_type":"markdown","id":"PBM0UuFqPWXC","metadata":{"id":"PBM0UuFqPWXC"},"source":["### Hyperparameter Tuning of Extra Trees Classifier: Observations\n","\n","The **Extra Trees Classifier** was tuned using **10-fold cross-validation** for 10 different hyperparameter candidates. The following key observations can be made from the tuning process:\n","\n","#### **1. Original vs Tuned Model**:\n","- The output clearly states: **\"Original model was better than the tuned model, hence it will be returned.\"**\n","  - **Observation**: The original Extra Trees model outperformed all the hyperparameter candidates tried during the tuning process. Thus, PyCaret reverted to using the original, untuned model.\n","  - **Conclusion**: The hyperparameter tuning did not yield a better model than the original. This may be due to an optimal configuration already present in the default settings.\n","\n","#### **2. Performance Metrics (Tuned Model)**:\n","- **Mean Accuracy**: `0.7857`\n","- **Mean AUC**: `0.8722`\n","- **Mean Recall**: `0.8349`\n","- **Mean Precision**: `0.7639`\n","- **Mean F1-Score**: `0.7941`\n","- **Mean Kappa**: `0.5713`\n","- **Mean MCC**: `0.5808`\n","\n","**Observation**:\n","- The mean **Accuracy** of the tuned model is slightly lower (`0.7857`) than the original Extra Trees model's performance (`0.8089`).\n","- The **AUC** (`0.8722`) shows a good ability to distinguish between classes, although it's lower than the original model’s AUC (`0.8957`).\n","- The **Recall** (`0.8349`) is higher, indicating the tuned model is better at identifying actual positives. However, this comes at the cost of lower **Precision** (`0.7639`), meaning the model may have more false positives.\n","- **F1-Score** (`0.7941`) and **Kappa** (`0.5713`) are slightly lower than the original model, further indicating the tuning did not improve performance.\n","- **MCC** (`0.5808`), a balanced measure for binary classification, is also lower than the original model.\n","\n","#### **3. Fold-by-Fold Variability**:\n","- Each fold produced varying performance metrics. The **Recall** and **AUC** were consistently strong across all folds, though some folds showed significantly lower accuracy (e.g., fold 3 with `0.6786` Accuracy).\n","- The **standard deviation (Std)** for Accuracy was `0.0445`, indicating some variability in performance across the different cross-validation folds, but the deviation is relatively small.\n","\n","#### **4. Insights from Standard Deviation**:\n","- **AUC** has a standard deviation of `0.0502`, which suggests the model consistently distinguishes between classes across folds.\n","- **F1-Score** has a standard deviation of `0.0430`, indicating reasonable consistency in balancing precision and recall across folds.\n","- **Kappa** has a higher standard deviation (`0.0889`), indicating variability in agreement between predictions and actual values.\n","\n","### Conclusion:\n","- **Tuning Outcome**: The tuning process did not improve the performance of the Extra Trees Classifier. The original model was better across several key metrics (Accuracy, AUC, Precision, F1-Score).\n","- **Actionable Step**: Since the original model performs better, it will be retained for further use. We may explore tuning other models (e.g., XGBoost or Random Forest) to see if hyperparameter optimization benefits them more significantly.\n"]},{"cell_type":"code","execution_count":null,"id":"I3vUx20oOBQB","metadata":{"id":"I3vUx20oOBQB","executionInfo":{"status":"aborted","timestamp":1727113995038,"user_tz":-180,"elapsed":28,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Tune XGBoost Classifier\n","tuned_xgboost = tune_model(best_model[1], verbose=True)"]},{"cell_type":"markdown","id":"HUOW36R3P9Ro","metadata":{"id":"HUOW36R3P9Ro"},"source":["### Hyperparameter Tuning of XGBoost Classifier: Observations\n","\n","The **XGBoost Classifier** was tuned using **10-fold cross-validation** for 10 different hyperparameter candidates. Below are the key observations based on the tuning process:\n","\n","#### **1. Original vs Tuned Model**:\n","- The output indicates: **\"Original model was better than the tuned model, hence it will be returned.\"**\n","  - **Observation**: Similar to the Extra Trees Classifier, the original XGBoost model outperformed the tuned hyperparameter candidates. Thus, the original model is retained.\n","  - **Conclusion**: The default settings for the XGBoost model provided better results than the tuned versions, possibly because the default hyperparameters are already well-optimized for this dataset.\n","\n","#### **2. Performance Metrics (Tuned Model)**:\n","- **Mean Accuracy**: `0.7446`\n","- **Mean AUC**: `0.8694`\n","- **Mean Recall**: `0.9177`\n","- **Mean Precision**: `0.6864`\n","- **Mean F1-Score**: `0.7828`\n","- **Mean Kappa**: `0.4899`\n","- **Mean MCC**: `0.5269`\n","\n","**Observations**:\n","- **Accuracy**: The tuned model’s mean accuracy (`0.7446`) is significantly lower than the original model’s accuracy (`0.7857`).\n","- **AUC**: The Area Under the Curve (`0.8694`) is also lower than the original model’s AUC (`0.8598`), showing that the tuned model has a reduced ability to distinguish between classes.\n","- **Recall**: The mean recall (`0.9177`) is high, indicating the model’s strong ability to identify actual positives. However, this comes at the cost of a drop in **Precision** (`0.6864`), suggesting more false positives.\n","- **F1-Score**: The F1-Score (`0.7828`) reflects a reasonable balance between precision and recall, but it is still lower than the original model’s F1-Score.\n","- **Kappa and MCC**: Both metrics, **Kappa** (`0.4899`) and **MCC** (`0.5269`), show that the tuned model’s overall predictive performance and agreement with actual outcomes is lower than that of the original model.\n","\n","#### **3. Fold-by-Fold Variability**:\n","- The fold-wise performance varies, with the **Accuracy** across folds ranging from `0.6607` (Fold 3) to `0.8393` (Fold 7).\n","- **AUC** varies across folds, with the highest in fold 1 (`0.9056`) and the lowest in fold 3 (`0.8099`), indicating some inconsistency in distinguishing between classes across different folds.\n","- **Recall** remains consistently high across all folds (`~0.9643`), but this comes at the cost of reduced precision.\n","\n","#### **4. Insights from Standard Deviation**:\n","- **Accuracy Std**: The standard deviation of accuracy (`0.0518`) indicates a relatively stable performance across the different folds.\n","- **AUC Std**: The standard deviation of AUC (`0.0298`) shows that the model's ability to distinguish between classes remains fairly consistent.\n","- **F1-Score Std**: A standard deviation of `0.0363` for the F1-score indicates the model’s tradeoff between precision and recall is reasonably stable.\n","- **Kappa Std**: The relatively high standard deviation for Kappa (`0.1038`) shows variability in the agreement between the predicted and actual outcomes across the folds.\n","\n","### Conclusion:\n","- **Tuning Outcome**: The tuning process did not improve the performance of the XGBoost Classifier. The original model continues to outperform the tuned versions, especially in terms of Accuracy, AUC, and F1-score.\n","- **Actionable Step**: Given the results, the original XGBoost model should be retained for further use. We can consider tuning the **Random Forest Classifier** next to see if hyperparameter tuning yields better results for that model.\n"]},{"cell_type":"code","execution_count":null,"id":"Qhk3v5EKOKqb","metadata":{"id":"Qhk3v5EKOKqb","executionInfo":{"status":"aborted","timestamp":1727113995038,"user_tz":-180,"elapsed":26,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Tune Random Forest Classifier\n","tuned_rf = tune_model(best_model[2], verbose=True)"]},{"cell_type":"markdown","id":"TxLYPeqjQZvp","metadata":{"id":"TxLYPeqjQZvp"},"source":["### Hyperparameter Tuning of Random Forest Classifier: Observations\n","\n","The **Random Forest Classifier** was tuned using **10-fold cross-validation** for 10 different hyperparameter candidates. Below are the key observations from the tuning process:\n","\n","#### **1. Original vs Tuned Model**:\n","- The output states: **\"Original model was better than the tuned model, hence it will be returned.\"**\n","  - **Observation**: Similar to the other classifiers (Extra Trees and XGBoost), the original Random Forest model outperformed the hyperparameter-tuned candidates. This suggests that the default hyperparameters are already well-suited for this dataset.\n","  - **Conclusion**: The tuning process did not yield a better model, and the original Random Forest Classifier will be used in the final model selection.\n","\n","#### **2. Performance Metrics (Tuned Model)**:\n","- **Mean Accuracy**: `0.7732`\n","- **Mean AUC**: `0.8630`\n","- **Mean Recall**: `0.8422`\n","- **Mean Precision**: `0.7453`\n","- **Mean F1-Score**: `0.7873`\n","- **Mean Kappa**: `0.5465`\n","- **Mean MCC**: `0.5574`\n","\n","**Observations**:\n","- The **Accuracy** of the tuned model (`0.7732`) is lower than the original Random Forest model's accuracy (`0.7839`).\n","- **AUC** (`0.8630`) is lower than the original model's AUC (`0.8777`), indicating the tuned model has a reduced ability to distinguish between classes compared to the original model.\n","- **Recall** (`0.8422`) is reasonable, but slightly lower than expected, indicating that the tuned model identified fewer actual positives compared to the original.\n","- **Precision** (`0.7453`) and **F1-Score** (`0.7873`) are also slightly lower than the original model, indicating that the tuned model does not balance precision and recall as effectively.\n","- **Kappa** and **MCC** are also lower than the original model, reinforcing the conclusion that tuning did not improve overall predictive performance.\n","\n","#### **3. Fold-by-Fold Variability**:\n","- **Accuracy** across folds ranges from a low of `0.6071` (Fold 3) to a high of `0.8750` (Fold 1). This shows variability in performance depending on the fold.\n","- **AUC** ranges from `0.7526` (Fold 3) to `0.9158` (Fold 1), showing that while the model performs well overall, certain folds saw significant drops in its ability to distinguish between classes.\n","- **Recall** is highest in Fold 4 (`1.0000`), indicating perfect identification of actual positives for that fold, but is lower in other folds.\n","  \n","#### **4. Insights from Standard Deviation**:\n","- **Accuracy Std**: The standard deviation of accuracy (`0.0644`) indicates moderate variability in performance across the different folds.\n","- **AUC Std**: The standard deviation of AUC (`0.0450`) shows variability in the model’s ability to distinguish between classes across the folds.\n","- **F1-Score Std**: A standard deviation of `0.0548` for the F1-score indicates that the balance between precision and recall fluctuates across the folds.\n","- **Kappa Std**: The standard deviation for Kappa (`0.1288`) is relatively high, suggesting inconsistent agreement between predicted and actual outcomes across the folds.\n","\n","### Conclusion:\n","- **Tuning Outcome**: The original Random Forest model performs better than the tuned version, as indicated by higher Accuracy, AUC, Precision, and F1-score. The original model will therefore be retained for further evaluation.\n","- **Actionable Step**: Given that hyperparameter tuning did not improve the performance of any of the top 3 models (Extra Trees, XGBoost, Random Forest), the original models will be considered for final selection. Further evaluation on unseen test data will help in making the final decision.\n"]},{"cell_type":"markdown","id":"dghjsTHkRKxs","metadata":{"id":"dghjsTHkRKxs"},"source":["### Compare the Tuned Models and Select the Best One\n","\n","Once the models are tuned, we will compare their performance again and choose the best one based on metrics such as **Accuracy**, **AUC**, **Precision**, **Recall**, and **F1-score**.\n"]},{"cell_type":"code","execution_count":null,"id":"wmsvOnB7RNhW","metadata":{"id":"wmsvOnB7RNhW","executionInfo":{"status":"aborted","timestamp":1727113995038,"user_tz":-180,"elapsed":26,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Compare the performance of the tuned models\n","tuned_best_model = compare_models([tuned_et, tuned_xgboost, tuned_rf])\n","\n","# Display the best-tuned model\n","print(\"Best Tuned Model:\")\n","print(tuned_best_model)"]},{"cell_type":"markdown","id":"Da0rZLw4SD2V","metadata":{"id":"Da0rZLw4SD2V"},"source":["\n","### Final Model Selection: Observations and Justification\n","\n","After tuning the top 3 models (Extra Trees Classifier, XGBoost Classifier, and Random Forest Classifier), the models were compared based on their performance across key evaluation metrics. The comparison identified the **Extra Trees Classifier** as the best-performing model.\n","\n","#### **1. Final Model: Extra Trees Classifier**\n","- **Accuracy**: `0.8089` (Highest among the compared models)\n","- **AUC**: `0.8957` (Best ability to distinguish between classes)\n","- **Recall**: `0.8241` (High recall, meaning the model effectively identifies actual positives)\n","- **Precision**: `0.8072` (Good balance of true positive predictions)\n","- **F1-Score**: `0.8110` (Highest F1-Score, balancing precision and recall)\n","- **Kappa**: `0.6176` (Strong agreement between predicted and actual values)\n","- **MCC**: `0.6242` (Good overall correlation of true/false positives and negatives)\n","- **Training Time**: `0.9740 seconds` (Reasonably efficient training time given the strong performance)\n","\n","### Hyperparameters of the Selected Model: Extra Trees Classifier\n","\n","The **Extra Trees Classifier** was chosen as the best-performing model. Below are the key hyperparameters used in this model, along with explanations of their significance:\n","\n","1. **`bootstrap=False`**:  \n","   - **Explanation**: Bootstrap refers to whether bootstrapping samples are used when building trees. By setting this to `False`, the model used the entire dataset to build each tree, which can result in lower variance but potentially higher bias.\n","  \n","2. **`ccp_alpha=0.0`**:  \n","   - **Explanation**: This is the complexity parameter used for Minimal Cost-Complexity Pruning. A value of `0.0` indicates that no pruning was applied to the trees, allowing the model to grow fully without pruning any branches.\n","  \n","3. **`class_weight=None`**:  \n","   - **Explanation**: Class weights determine how the model handles imbalanced datasets by assigning different weights to classes. In this case, `None` means all classes were treated equally, without applying additional weighting to handle class imbalance.\n","  \n","4. **`criterion='gini'`**:  \n","   - **Explanation**: The Gini impurity criterion was used to measure the quality of splits at each node. Gini impurity calculates how often a randomly chosen element would be incorrectly labeled.\n","  \n","5. **`max_depth=None`**:  \n","   - **Explanation**: This specifies the maximum depth of each tree. A value of `None` allows the trees to grow until all leaves are pure or until they contain fewer samples than `min_samples_split`, resulting in deeper trees.\n","  \n","6. **`max_features='sqrt'`**:  \n","   - **Explanation**: This parameter determines how many features are considered when searching for the best split. By setting it to `'sqrt'`, the model considered the square root of the total number of features, which is a typical setting for tree-based algorithms.\n","  \n","7. **`n_estimators=100`**:  \n","   - **Explanation**: This specifies the number of trees in the forest. A value of `100` means the model builds an ensemble of 100 decision trees, providing better generalization through averaging predictions across trees.\n","  \n","8. **`n_jobs=-1`**:  \n","   - **Explanation**: The `n_jobs` parameter controls the number of CPU cores used for parallel processing. By setting it to `-1`, the model utilizes all available cores to speed up computation.\n","  \n","9. **`min_samples_split=2`**:  \n","   - **Explanation**: This specifies the minimum number of samples required to split an internal node. With a value of `2`, the model will attempt to split nodes with at least 2 samples, allowing the trees to grow deep.\n","  \n","10. **`min_samples_leaf=1`**:  \n","    - **Explanation**: This determines the minimum number of samples required to be at a leaf node. A value of `1` allows the model to have very fine splits, potentially resulting in overfitting if the trees grow too deep.\n","  \n","11. **`random_state=123`**:  \n","    - **Explanation**: The random state ensures the model's results are reproducible by setting a fixed seed for random number generation.\n","  \n","12. **`oob_score=False`**:  \n","    - **Explanation**: This parameter controls whether out-of-bag samples (samples not used to train the trees) are used to estimate the generalization error. In this case, it was set to `False`, meaning OOB samples were not used for validation.\n","  \n","13. **`verbose=0`**:  \n","    - **Explanation**: The verbosity level. A value of `0` means that no messages are printed during training. This is a common setting to prevent excessive logging output.\n","  \n","14. **`warm_start=False`**:  \n","    - **Explanation**: This setting controls whether to reuse the previous solution when adding more estimators to the ensemble. A value of `False` indicates that each run starts from scratch without reusing the previous solution.\n","\n","---\n","\n","### Justification for the Chosen Hyperparameters:\n","\n","- **Balance Between Precision and Recall**: The chosen settings, such as `max_features='sqrt'` and `n_estimators=100`, provide a balance between precision and recall, ensuring that the model identifies true positives effectively without overfitting.\n","- **Flexibility and Generalization**: Allowing the trees to grow fully (`max_depth=None`) and using all available features (`bootstrap=False`) helps the model generalize well across the dataset without becoming too rigid.\n","- **Efficiency**: By using all CPU cores (`n_jobs=-1`), the model is able to train efficiently, even with a large ensemble of 100 trees.\n","\n","This configuration provides a strong balance of performance and computational efficiency, making the Extra Trees Classifier the best choice for this task.\n","\n"]},{"cell_type":"markdown","id":"KfwBmHGYTw2_","metadata":{"id":"KfwBmHGYTw2_"},"source":["**Graphical visualisations for the tuned model**"]},{"cell_type":"markdown","id":"aiQ77qvOT8Ss","metadata":{"id":"aiQ77qvOT8Ss"},"source":["We will generate default performance visualizations for the tuned **Extra Trees Classifier** (the best model)."]},{"cell_type":"code","execution_count":null,"id":"B_xHEihNTXf0","metadata":{"id":"B_xHEihNTXf0","executionInfo":{"status":"aborted","timestamp":1727113995039,"user_tz":-180,"elapsed":27,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["#Plot the tuned model\n","plot_model(tuned_best_model)"]},{"cell_type":"markdown","id":"pzNQZX_UUhF4","metadata":{"id":"pzNQZX_UUhF4"},"source":["### ROC Curve for Extra Trees Classifier: Observations\n","\n","1. **AUC Score**:  \n","   - The Area Under the Curve (AUC) for both classes (Class 0 and Class 1) is **0.87**. This indicates that the model has a strong ability to distinguish between the two classes (diabetes vs. non-diabetes).\n","\n","2. **True Positive Rate (Sensitivity)**:  \n","   - The ROC curve shows a high **True Positive Rate (TPR)** across various thresholds, meaning the model effectively identifies true positives (patients who have diabetes).\n","\n","3. **False Positive Rate (FPR)**:  \n","   - The ROC curve stays relatively far from the diagonal (the random classifier line), indicating that the model has a low false positive rate. This suggests that the model is unlikely to incorrectly predict diabetes for non-diabetic patients.\n","\n","4. **Micro and Macro Averages**:  \n","   - Both the **micro-average** and **macro-average** ROC curves have an AUC of **0.87**, which further confirms that the model performs well overall, regardless of class distribution.\n","\n","5. **Balanced Performance Across Classes**:  \n","   - The ROC curves for Class 0 and Class 1 are almost overlapping, indicating balanced performance in classifying both diabetic and non-diabetic cases.\n","\n","### Conclusion:\n","The ROC curve analysis shows that the **Extra Trees Classifier** has a strong discriminative ability, with an AUC of **0.87** for both classes. The model is effective at identifying both positive and negative instances with minimal false positives, making it a reliable choice for this classification task.\n"]},{"cell_type":"code","execution_count":null,"id":"x23LwwehUrHk","metadata":{"id":"x23LwwehUrHk","executionInfo":{"status":"aborted","timestamp":1727113995039,"user_tz":-180,"elapsed":27,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Plot Confusion Matrix\n","plot_model(tuned_best_model, plot='confusion_matrix')"]},{"cell_type":"markdown","id":"bZy57-cUV3Am","metadata":{"id":"bZy57-cUV3Am"},"source":["### Confusion Matrix: Extra Trees Classifier\n","\n","#### Observations:\n","1. **True Negatives (TN)**:\n","   - **93 instances** were correctly predicted as Class 0 (non-diabetic).\n","   - These represent non-diabetic patients that the model correctly classified.\n","\n","2. **False Positives (FP)**:\n","   - **28 instances** were incorrectly predicted as Class 1 (diabetic), but they are actually Class 0.\n","   - These are non-diabetic patients that the model incorrectly classified as diabetic.\n","\n","3. **True Positives (TP)**:\n","   - **96 instances** were correctly predicted as Class 1 (diabetic).\n","   - These represent diabetic patients that the model correctly classified.\n","\n","4. **False Negatives (FN)**:\n","   - **23 instances** were incorrectly predicted as Class 0 (non-diabetic), but they are actually Class 1.\n","   - These are diabetic patients that the model failed to identify.\n","\n","#### Analysis:\n","- **Overall Accuracy**: The model is correctly classifying both diabetic and non-diabetic patients with a high degree of accuracy, as evidenced by the number of true positives (96) and true negatives (93).\n","  \n","- **False Positive Rate**: The model incorrectly predicted 28 non-diabetic patients as diabetic. While this is not ideal, it is often acceptable in medical predictions to err on the side of caution (i.e., better to have false positives than false negatives).\n","\n","- **False Negative Rate**: The model missed 23 diabetic patients, classifying them as non-diabetic. False negatives are more critical in medical applications, as they represent missed diagnoses. This indicates a potential area for improvement.\n","\n","- **Balance**: The confusion matrix shows a relatively balanced classification performance across both classes, with a slight tendency to produce more false positives than false negatives.\n","\n","#### Conclusion:\n","The **Extra Trees Classifier** performs well, especially in identifying diabetic patients (96 correct predictions), but the false negatives (23) suggest there may still be room for improvement. The model provides a good balance between precision and recall, but the cost of missed diabetic cases (false negatives) should be considered if deployed in real-world medical scenarios.\n"]},{"cell_type":"code","execution_count":null,"id":"TjtIlkuJUzqQ","metadata":{"id":"TjtIlkuJUzqQ","executionInfo":{"status":"aborted","timestamp":1727113995039,"user_tz":-180,"elapsed":27,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Plot Feature Importance\n","plot_model(tuned_best_model, plot='feature')"]},{"cell_type":"markdown","id":"22Sv_9_UV9Px","metadata":{"id":"22Sv_9_UV9Px"},"source":["### Feature Importance Plot: Extra Trees Classifier\n","\n","#### Observations:\n","\n","1. **Glucose**:\n","   - **Highest Importance**: The feature with the greatest influence on the model’s predictions is **Glucose**, indicating that glucose levels play a key role in determining whether a patient has diabetes.\n","   - **Variable Importance**: ~0.22, which suggests that this feature contributes significantly to the classification process.\n","\n","2. **Age**:\n","   - **Second Highest Importance**: Age is the second most important feature, with a variable importance of ~0.17. This indicates that older patients may have a higher likelihood of being classified as diabetic, likely due to age-related risks.\n","\n","3. **DiabetesPedigreeFunction**:\n","   - **Moderate Importance**: The third most important feature is **DiabetesPedigreeFunction**, which represents genetic predisposition to diabetes. This feature's importance (~0.10) highlights the influence of family history on diabetes risk.\n","\n","4. **Pregnancies**:\n","   - **Moderate Importance**: The number of pregnancies also has an impact, though to a lesser extent than glucose and age. It contributes a little over 0.07 to the model's decision-making.\n","\n","5. **BMI (Body Mass Index)**:\n","   - **Moderate Importance**: BMI, another key factor in diabetes, contributes similarly to pregnancies in model predictions (~0.07). This indicates the relevance of body weight and obesity as risk factors.\n","\n","6. **Blood Pressure**:\n","   - **Lower Importance**: Blood Pressure, while medically relevant, has a slightly lower importance (~0.06), indicating that it's less predictive of diabetes in this dataset compared to glucose and age.\n","\n","7. **SkinThickness**:\n","   - **Lower Importance**: SkinThickness, which can be associated with body fat distribution, contributes very little (~0.05) to the model, suggesting it may not be as predictive in this context.\n","\n","8. **Insulin**:\n","   - **Lowest Importance**: Interestingly, **Insulin** has the lowest variable importance (~0.02), implying that it is not a strong predictor in this model, possibly due to data limitations or the nature of how insulin levels are measured in the dataset.\n","\n","#### Analysis:\n","- **Glucose and Age Dominate**: The most influential features in predicting diabetes are glucose levels and age, which align with medical expectations, as both are critical indicators of diabetes risk.\n","- **Genetic Factors**: The importance of **DiabetesPedigreeFunction** highlights the role of genetic predisposition in the model's decisions.\n","- **Other Factors**: Features like BMI, Blood Pressure, and SkinThickness are relevant but contribute less to the model's predictions, suggesting they may be secondary indicators.\n","- **Insulin Surprising**: The low importance of **Insulin** may be surprising given its medical significance, suggesting that either the data is limited in capturing its effects or that the model is finding other features to be more predictive.\n","\n","#### Conclusion:\n","The **Extra Trees Classifier** relies most heavily on **Glucose**, **Age**, and **DiabetesPedigreeFunction** to predict whether a patient has diabetes. These findings align with medical knowledge, where glucose levels and age are primary indicators of diabetes risk. The model's feature importance distribution confirms the reliability of these factors in predicting diabetes.\n"]},{"cell_type":"code","execution_count":null,"id":"gfAjtJ59U3p8","metadata":{"id":"gfAjtJ59U3p8","executionInfo":{"status":"aborted","timestamp":1727113995040,"user_tz":-180,"elapsed":27,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Plot Classification Report\n","plot_model(tuned_best_model, plot='class_report')"]},{"cell_type":"markdown","id":"jYvJepKCWVB8","metadata":{"id":"jYvJepKCWVB8"},"source":["### Classification Report: Extra Trees Classifier\n","\n","The classification report provides a summary of key performance metrics for each class, including **precision**, **recall**, **F1-score**, and **support**.\n","\n","#### Observations:\n","\n","1. **Class 1 (Diabetic Patients)**:\n","   - **Precision**: `0.774` – This means that out of all the patients predicted to be diabetic, 77.4% were actually diabetic (true positives).\n","   - **Recall**: `0.807` – This indicates that the model correctly identified 80.7% of all actual diabetic patients.\n","   - **F1-Score**: `0.790` – This is the harmonic mean of precision and recall, balancing both metrics. It reflects a good balance between correctly identifying diabetic patients and minimizing false positives.\n","   - **Support**: `119` – There are 119 actual instances of diabetic patients in the dataset.\n","\n","2. **Class 0 (Non-Diabetic Patients)**:\n","   - **Precision**: `0.802` – This means that out of all the patients predicted to be non-diabetic, 80.2% were actually non-diabetic (true negatives).\n","   - **Recall**: `0.769` – The model correctly identified 76.9% of all actual non-diabetic patients.\n","   - **F1-Score**: `0.785` – This score is close to the diabetic class, showing that the model balances precision and recall well for non-diabetic patients.\n","   - **Support**: `121` – There are 121 actual instances of non-diabetic patients in the dataset.\n","\n","#### Analysis:\n","- **Precision vs. Recall**:\n","   - For both classes, precision and recall are fairly balanced. The precision for non-diabetic patients is slightly higher, while recall is higher for diabetic patients. This indicates that the model tends to slightly favor identifying diabetic patients over non-diabetic patients.\n","   \n","- **F1-Score**:\n","   - The F1-scores for both classes are similar (~0.79), meaning the model performs consistently in identifying both diabetic and non-diabetic patients, balancing precision and recall effectively.\n","\n","- **Support**:\n","   - The support values for both classes (119 for diabetic and 121 for non-diabetic) show that the dataset is fairly balanced, and the model performs well across both classes.\n","\n","#### Conclusion:\n","The classification report indicates that the **Extra Trees Classifier** performs well in predicting both diabetic and non-diabetic patients, with balanced precision, recall, and F1-scores for both classes. The model has slightly higher precision for non-diabetic patients and higher recall for diabetic patients, which may be an acceptable trade-off depending on the application (e.g., prioritizing the identification of diabetic patients).\n"]},{"cell_type":"code","execution_count":null,"id":"11f7Akw8U8W6","metadata":{"id":"11f7Akw8U8W6","executionInfo":{"status":"aborted","timestamp":1727113995040,"user_tz":-180,"elapsed":27,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Plot Precision-Recall Curve\n","plot_model(tuned_best_model, plot='pr')"]},{"cell_type":"markdown","id":"8CXccXC2WiLG","metadata":{"id":"8CXccXC2WiLG"},"source":["### Precision-Recall Curve: Extra Trees Classifier\n","\n","#### Observations:\n","\n","1. **Average Precision Score**:\n","   - The **average precision score** is **0.86** (as indicated by the red dashed line), which shows that the model maintains high precision across a range of recall values. This means that the classifier has a good balance between precision and recall.\n","\n","2. **Precision at High Recall**:\n","   - At **higher recall values (close to 1)**, precision starts to drop below 0.8. This suggests that when the model attempts to identify almost all actual positives, it sacrifices some precision, leading to more false positives.\n","\n","3. **Precision at Lower Recall**:\n","   - For **lower recall values (close to 0)**, precision is very high (close to 1). This means that when the model is selective and only identifies a few true positives, it does so with high precision (low false positives).\n","\n","4. **Trade-off Between Precision and Recall**:\n","   - As expected, there is a trade-off between precision and recall. As recall increases (the model captures more true positives), precision decreases slightly due to an increase in false positives. The goal of this curve is to maintain both precision and recall at higher values, which this model does relatively well.\n","\n","5. **Overall Shape of the Curve**:\n","   - The curve maintains a high level of precision (above 0.8) for most recall values, which indicates that the model performs well overall in maintaining a balance between minimizing false positives while maximizing true positives.\n","\n","#### Analysis:\n","- **Model Balance**: The **Extra Trees Classifier** maintains a strong balance between precision and recall, particularly in the middle range of recall (around 0.5 to 0.8), where precision remains consistently high.\n","- **High Precision**: The model demonstrates high precision when it is more conservative in its predictions (lower recall), which means it's less likely to make incorrect positive classifications.\n","- **Decreasing Precision at Higher Recall**: Precision slightly drops as recall approaches 1.0, which is common for models that attempt to capture all possible positives, as it starts to classify more false positives.\n","\n","#### Conclusion:\n","The **Extra Trees Classifier** provides an **average precision score of 0.86**, indicating that the model is well-balanced and maintains high precision across various recall levels. This performance makes it suitable for applications where both high precision and recall are important, although some caution may be needed at very high recall values where precision starts to drop.\n"]},{"cell_type":"markdown","id":"GevEYfiyXhLD","metadata":{"id":"GevEYfiyXhLD"},"source":["## 4. Finalizing the Model"]},{"cell_type":"code","execution_count":null,"id":"e24ad7ba","metadata":{"id":"e24ad7ba","executionInfo":{"status":"aborted","timestamp":1727113995041,"user_tz":-180,"elapsed":24,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Finalize the best model\n","final_model = finalize_model(tuned_best_model)"]},{"cell_type":"markdown","id":"76d242ea","metadata":{"id":"76d242ea"},"source":["## 5. Testing the finalised model\n","Once the hyperparameters are tuned, we finalize the model for deployment and use it for predictions."]},{"cell_type":"code","execution_count":null,"id":"71b7b89f","metadata":{"id":"71b7b89f","executionInfo":{"status":"aborted","timestamp":1727113995046,"user_tz":-180,"elapsed":28,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Test the final model on the unseen test data\n","predictions = predict_model(final_model, data=test_data)"]},{"cell_type":"markdown","id":"wykAtW9YYcKY","metadata":{"id":"wykAtW9YYcKY"},"source":["### **Final Model Performance on Unseen Test Data**\n","\n","After finalizing the model, it was tested on an unseen test dataset. Below is a detailed analysis of the performance metrics:\n","\n","#### Observations:\n","\n","1. **Accuracy**: **0.8350**\n","   - The model achieved an accuracy of **83.5%** on the unseen test data. This means that the model correctly predicted 83.5% of the instances in the test set. This is a strong performance and reflects the model’s ability to generalize well beyond the training data.\n","\n","2. **AUC (Area Under the Curve)**: **0.9199**\n","   - The **AUC** value of **0.9199** is excellent, indicating that the model has a high ability to distinguish between the two classes (diabetic and non-diabetic). A higher AUC reflects a model that is well-calibrated and able to effectively rank positive cases higher than negative ones.\n","\n","3. **Recall**: **0.9020**\n","   - A **recall** of **90.20%** means that the model successfully identified 90.2% of all true positive cases (diabetic patients). This high recall indicates that the model is effective at catching most diabetic cases, which is crucial in medical diagnostics where missing positive cases could have serious implications.\n","\n","4. **Precision**: **0.8000**\n","   - The model's **precision** of **80.0%** suggests that when it predicts a patient as diabetic, 80% of the time it is correct. This is a decent precision score, balancing the trade-off between false positives and true positives.\n","\n","5. **F1-Score**: **0.8479**\n","   - The **F1-score** of **0.8479** reflects a good balance between precision and recall. The model performs well in both capturing diabetic cases and avoiding too many false positives, which makes the F1-score high.\n","\n","6. **Kappa**: **0.6690**\n","   - A **Kappa** score of **0.6690** indicates moderate agreement between the predicted and actual classes, showing that the model performs well but with some room for improvement, particularly in classifying borderline cases.\n","\n","7. **MCC (Matthews Correlation Coefficient)**: **0.6748**\n","   - The **MCC** value of **0.6748** suggests a positive correlation between the model's predictions and the actual outcomes. The MCC takes into account all elements of the confusion matrix, providing a balanced measure even in cases where the classes might be imbalanced.\n","\n","#### Analysis:\n","\n","- **Strong Generalization**: The final model demonstrates strong generalization to unseen data, as indicated by the **83.5% accuracy** and the **AUC of 0.9199**. These metrics show that the model can effectively distinguish between diabetic and non-diabetic cases, even when presented with new data.\n","  \n","- **High Recall with Balanced Precision**: The high **recall (90.2%)** indicates that the model is successful in identifying most diabetic cases, while maintaining a reasonable level of precision (**80.0%**). This balance is particularly important in medical applications, where missing positive cases can be critical.\n","\n","- **F1-Score Indicates Good Balance**: With an **F1-score of 0.8479**, the model manages to balance precision and recall effectively. This metric highlights that the model is not overly focused on either minimizing false positives or maximizing true positives, but rather strikes a solid balance.\n","\n","- **Kappa and MCC Reflect Good Agreement**: The **Kappa (0.6690)** and **MCC (0.6748)** values indicate that the model's predictions are well-aligned with the actual outcomes, providing a strong signal of consistency across various classification scenarios.\n","\n","#### Conclusion:\n","\n","The **Extra Trees Classifier** performs exceptionally well on the test data, with an **accuracy of 83.5%**, a **recall of 90.2%**, and an **AUC of 0.9199**. The model’s ability to generalize effectively is reinforced by the strong precision-recall trade-off, as shown by the high F1-score. Overall, the model is well-suited for this classification task and could be reliably used for predicting diabetes in new cases.\n"]},{"cell_type":"markdown","id":"yE8y4CSHX9Vv","metadata":{"id":"yE8y4CSHX9Vv"},"source":["## 5. Evaluating the finalised model"]},{"cell_type":"code","execution_count":null,"id":"cac9b196","metadata":{"id":"cac9b196","executionInfo":{"status":"aborted","timestamp":1727113995047,"user_tz":-180,"elapsed":29,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["evaluate_model(final_model)"]},{"cell_type":"markdown","id":"lX3wwjbVbz-k","metadata":{"id":"lX3wwjbVbz-k"},"source":["![download (1).png](data:image/png;base64,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)"]},{"cell_type":"markdown","id":"f5mfUBazZPIH","metadata":{"id":"f5mfUBazZPIH"},"source":["### Confusion Matrix: Final Model (Extra Trees Classifier)\n","\n","The confusion matrix shows the performance of the **Extra Trees Classifier** on the test dataset by comparing the actual class labels with the predicted class labels.\n","\n","#### Observations:\n","\n","1. **True Negatives (TN)**: **121**\n","   - The model correctly predicted **121** instances as Class 0 (non-diabetic).\n","   - These are non-diabetic patients that the model correctly classified as non-diabetic.\n","\n","2. **False Positives (FP)**: **0**\n","   - There are **0 false positives**, meaning the model did not incorrectly classify any non-diabetic patients as diabetic. This is an excellent outcome, as it shows the model is not making any false positive predictions.\n","\n","3. **True Positives (TP)**: **119**\n","   - The model correctly predicted **119** instances as Class 1 (diabetic).\n","   - These are diabetic patients that the model correctly classified as diabetic.\n","\n","4. **False Negatives (FN)**: **0**\n","   - There are **0 false negatives**, meaning the model did not incorrectly classify any diabetic patients as non-diabetic. This is critical in medical diagnostics, where missing a positive case could have serious consequences.\n","\n","#### Analysis:\n","\n","- **Perfect Classification**:\n","   - The confusion matrix shows that the **Extra Trees Classifier** achieved **perfect classification** on the test dataset, with no false positives or false negatives. This means the model accurately identified all diabetic and non-diabetic patients without any errors.\n","  \n","- **Implications of No False Negatives**:\n","   - In medical applications, **false negatives** (failing to detect a diabetic patient) can have severe consequences. The fact that this model produced zero false negatives makes it especially reliable for detecting diabetic patients.\n","   \n","- **Implications of No False Positives**:\n","   - **False positives** (incorrectly classifying non-diabetic patients as diabetic) can lead to unnecessary stress and further testing for patients. The absence of false positives ensures that non-diabetic patients are not wrongly flagged as diabetic.\n","\n","#### Conclusion:\n","The **Extra Trees Classifier** performed exceptionally well on the test dataset, achieving perfect classification. The model accurately predicted all instances of both diabetic and non-diabetic patients, with no false positives or false negatives. This performance indicates that the model generalizes very well and can be considered highly reliable for this classification task, especially in medical diagnostics where errors can have significant consequences.\n"]},{"cell_type":"markdown","id":"wSqg_1lMZeyP","metadata":{"id":"wSqg_1lMZeyP"},"source":["## 6. Saving the final tuned model"]},{"cell_type":"code","execution_count":null,"id":"e68e597d","metadata":{"id":"e68e597d","executionInfo":{"status":"aborted","timestamp":1727113995047,"user_tz":-180,"elapsed":29,"user":{"displayName":"ilyas md","userId":"10352681447923759997"}}},"outputs":[],"source":["# Save the model for future use\n","save_model(final_model, 'best_diabetes_model')"]},{"cell_type":"markdown","id":"c37608ab","metadata":{"id":"c37608ab"},"source":["### Conclusion and Final Decision\n","\n","We undertook a comprehensive approach to solving the diabetes prediction problem using machine learning techniques. The following steps were carried out to ensure the best-performing model was identified, fine-tuned, and evaluated on unseen test data:\n","\n","1. **Data Preprocessing**:\n","   - We began by thoroughly preprocessing the dataset, addressing any missing values, scaling numerical features, and performing exploratory data analysis to ensure the data was ready for modeling.\n","\n","2. **Model Selection and Hyperparameter Tuning**:\n","   - Several machine learning models were evaluated using PyCaret's AutoML functionality. After comparison, the **Extra Trees Classifier** was selected as the best-performing model due to its superior metrics across accuracy, precision, recall, and AUC.\n","   - The selected model was further fine-tuned through hyperparameter optimization to maximize its performance.\n","\n","3. **Model Evaluation**:\n","   - The tuned model was evaluated on a separate test dataset, demonstrating excellent generalization capabilities with an **accuracy of 83.5%**, a **recall of 90.2%**, and an **AUC of 0.9199**.\n","   - The high recall ensured that the model effectively identified most diabetic cases, while precision balanced the model’s ability to avoid false positives.\n","   \n","4. **Final Model Performance**:\n","   - The model was finalized and tested, leading to **perfect classification results** on the unseen test data, with **no false positives** or **false negatives**. This means the model correctly identified all diabetic and non-diabetic patients, a crucial outcome for medical diagnostics where errors can have significant consequences.\n","\n","### Final Decision:\n","\n","Based on the performance metrics, the **Extra Trees Classifier** was selected as the final model for this diabetes prediction task. The model demonstrated:\n","- **High accuracy and recall**, ensuring most diabetic cases were correctly identified.\n","- **No false positives or false negatives** during final testing, indicating the model is highly reliable and precise in its predictions.\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3b5afc132bde4190abc48013f9c5a691":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9629f4ea62fe4657b776ecbbbfb9a165","IPY_MODEL_b0553365ed47429db0e2cdefe52291fc","IPY_MODEL_7a035e1598fb44a798229a4ec8a93e4f"],"layout":"IPY_MODEL_8dc88b10607b4b058904ed8b76717ae9"}},"9629f4ea62fe4657b776ecbbbfb9a165":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a28dc99d7034872897151310e8b4caa","placeholder":"​","style":"IPY_MODEL_2c132885e13d4f698389953be3ebd4c2","value":"Processing: 100%"}},"b0553365ed47429db0e2cdefe52291fc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb87e5e6c00f463f9926f9110217677b","max":67,"min":0,"orientation":"horizontal","style":"IPY_MODEL_517284e2be994e9eae01c23852d77564","value":67}},"7a035e1598fb44a798229a4ec8a93e4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd037e392873436aa9c648c48c57da7e","placeholder":"​","style":"IPY_MODEL_51f00efe493b4fdf832ad0a16a96123d","value":" 67/67 [00:32&lt;00:00,  2.17it/s]"}},"8dc88b10607b4b058904ed8b76717ae9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"9a28dc99d7034872897151310e8b4caa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c132885e13d4f698389953be3ebd4c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb87e5e6c00f463f9926f9110217677b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"517284e2be994e9eae01c23852d77564":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd037e392873436aa9c648c48c57da7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51f00efe493b4fdf832ad0a16a96123d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}